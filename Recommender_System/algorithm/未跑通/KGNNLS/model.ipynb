{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import tensorflow as tf\n",
    "from Recommender_System.algorithm.未跑通.KGCN.layer import SumAggregator, ConcatAggregator, NeighborAggregator\n",
    "from Recommender_System.algorithm.未跑通.KGNNLS.layer import LabelAggregator, HashLookupWrapper\n",
    "from Recommender_System.utility.decorator import logger\n",
    "\n",
    "\n",
    "@logger('初始化KGNNLS模型：', ('n_user', 'n_entity', 'n_relation', 'neighbor_size', 'iter_size', 'dim', 'l2', 'ls', 'aggregator'))\n",
    "def KGNNLS_model(n_user: int, n_entity: int, n_relation: int, adj_entity: List[List[int]], adj_relation: List[List[int]],\n",
    "                 interaction_table: tf.lookup.StaticHashTable, neighbor_size: int, iter_size=2, dim=16, l2=1e-7, ls=1.,\n",
    "                 aggregator='sum') -> tf.keras.Model:\n",
    "    assert neighbor_size == len(adj_entity[0]) == len(adj_relation[0])\n",
    "    l2 = tf.keras.regularizers.l2(l2)\n",
    "\n",
    "    user_id = tf.keras.Input(shape=(), name='user_id', dtype=tf.int32)\n",
    "    item_id = tf.keras.Input(shape=(), name='item_id', dtype=tf.int32)\n",
    "\n",
    "    user_embedding = tf.keras.layers.Embedding(n_user, dim, embeddings_initializer='glorot_uniform', embeddings_regularizer=l2)\n",
    "    entity_embedding = tf.keras.layers.Embedding(n_entity, dim, embeddings_initializer='glorot_uniform', embeddings_regularizer=l2)\n",
    "    relation_embedding = tf.keras.layers.Embedding(n_relation, dim, embeddings_initializer='glorot_uniform', embeddings_regularizer=l2)\n",
    "\n",
    "    u = user_embedding(user_id)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    entities = [tf.expand_dims(item_id, axis=1)]  # [(batch, 1), (batch, n_neighbor), (batch, n_neighbor^2), ..., (batch, n_neighbor^n_iter)]\n",
    "    relations = []  # [(batch, n_neighbor), (batch, n_neighbor^2), ..., (batch, n_neighbor^n_iter)]\n",
    "    for _ in range(iter_size):\n",
    "        neighbor_entities = flatten(tf.gather(adj_entity, entities[-1]))\n",
    "        neighbor_relations = flatten(tf.gather(adj_relation, entities[-1]))\n",
    "        entities.append(neighbor_entities)\n",
    "        relations.append(neighbor_relations)\n",
    "\n",
    "    if aggregator == 'sum':\n",
    "        aggregator_class = SumAggregator\n",
    "    elif aggregator == 'concat':\n",
    "        aggregator_class = ConcatAggregator\n",
    "    elif aggregator == 'neighbor':\n",
    "        aggregator_class = NeighborAggregator\n",
    "    else:\n",
    "        raise Exception(\"Unknown aggregator: \" + aggregator)\n",
    "\n",
    "    entity_vectors = [entity_embedding(entity) for entity in entities]  # [(batch, 1, dim), (batch, n_neighbor, dim), (batch, n_neighbor^2, dim), ..., (batch, n_neighbor^n_iter, dim)]\n",
    "    relation_vectors = [relation_embedding(relation) for relation in relations]  # [(batch, n_neighbor, dim), (batch, n_neighbor^2, dim), ..., (batch, n_neighbor^n_iter, dim)]\n",
    "    for it in range(iter_size):\n",
    "        aggregator = aggregator_class(activation='relu' if it < iter_size - 1 else 'tanh', kernel_regularizer=l2)\n",
    "        entities_next = []\n",
    "        for hop in range(iter_size - it):\n",
    "            inputs = (entity_vectors[hop], entity_vectors[hop + 1], relation_vectors[hop], u)\n",
    "            vector = aggregator(inputs, neighbor_size=neighbor_size)\n",
    "            entities_next.append(vector)\n",
    "        entity_vectors = entities_next\n",
    "    i = tf.reshape(entity_vectors[0], shape=(-1, dim))  # batch, dim\n",
    "    score = tf.sigmoid(tf.reduce_sum(u * i, axis=1))  # batch\n",
    "\n",
    "    # calculate initial labels; calculate updating masks for label propagation\n",
    "    entity_labels = []\n",
    "    reset_masks = []  # True means the label of this item is reset to initial value during label propagation\n",
    "    holdout_item_for_user = None\n",
    "    offset = tf.constant(10 ** len(str(n_entity)), dtype=tf.int64)\n",
    "    interaction_table_lookup = HashLookupWrapper(interaction_table)\n",
    "\n",
    "    for entities_per_iter in entities:\n",
    "        users = tf.cast(tf.expand_dims(user_id, axis=1), dtype=tf.int64)  # [batch_size, 1]\n",
    "        user_entity_concat = users * offset + tf.cast(entities_per_iter, dtype=tf.int64)  # [batch_size, n_neighbor^i]\n",
    "\n",
    "        if holdout_item_for_user is None:  # the first one in entities is the items to be held out\n",
    "            holdout_item_for_user = user_entity_concat  # [batch, 1]\n",
    "\n",
    "        initial_label = interaction_table_lookup(user_entity_concat)  # [batch_size, n_neighbor^i]\n",
    "        holdout_mask = tf.cast(holdout_item_for_user - user_entity_concat, tf.bool)  # False if the item is held out\n",
    "        reset_mask = tf.cast(initial_label - tf.constant(0.5), tf.bool)  # True if the entity is a labeled item\n",
    "        reset_mask = tf.logical_and(reset_mask, holdout_mask)  # remove held-out items\n",
    "        initial_label = tf.cast(holdout_mask, tf.keras.backend.floatx()) * initial_label + tf.cast(\n",
    "            tf.logical_not(holdout_mask), tf.keras.backend.floatx()) * tf.constant(0.5)  # label initialization\n",
    "\n",
    "        reset_masks.append(reset_mask)\n",
    "        entity_labels.append(initial_label)\n",
    "    reset_masks = reset_masks[:-1]  # we do not need the reset_mask for the last iteration\n",
    "\n",
    "    # label propagation\n",
    "    aggregator = LabelAggregator()\n",
    "    for it in range(iter_size):\n",
    "        entity_labels_next = []\n",
    "        for hop in range(iter_size - it):\n",
    "            inputs = (entity_labels[hop], entity_labels[hop + 1], relation_vectors[hop], u, reset_masks[hop])\n",
    "            vector = aggregator(inputs, neighbor_size=neighbor_size)\n",
    "            entity_labels_next.append(vector)\n",
    "        entity_labels = entity_labels_next\n",
    "    predicted_labels = tf.squeeze(entity_labels[0], axis=1)  # batch\n",
    "\n",
    "    label_keys = tf.cast(user_id, dtype=tf.int64) * offset + tf.cast(item_id, dtype=tf.int64)  # batch\n",
    "    labels = interaction_table_lookup(label_keys)  # batch\n",
    "    ls_loss = tf.keras.losses.binary_crossentropy(labels, predicted_labels)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[user_id, item_id], outputs=score)\n",
    "    model.add_loss(ls_loss * ls)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
